# Legal Fine-Tuning Project Summary & Future Improvement Guide

## Executive Summary

**Project Goal:** Fine-tune Qwen 2.5 32B on legal datasets to improve legal reasoning capabilities.

**Result:** **No significant improvement** (-0.11 points average). Fine-tuning did not improve legal knowledge.

**Key Finding:** 500 training steps insufficient. Need 10,000+ steps and better dataset alignment.

---

## What Was Achieved

### ‚úÖ Completed Work

1. **Fine-Tuning Setup**
   - Model: Qwen 2.5 32B Instruct (~62GB)
   - Hardware: 8x AMD MI300X GPUs (1.5TB HBM total)
   - Method: LoRA (Low-Rank Adaptation)
   - Dataset: `lamblamb/pile_of_law_subset`
   - Training: 500 steps (checkpoint-500 saved)

2. **Evaluation Framework**
   - Created comprehensive evaluation with 10 diverse legal questions
   - Scoring: 4 criteria (Knowledge Depth, Accuracy, Reasoning, Completeness)
   - Evaluation script: `scripts/evaluation/eval_legal_models_scientific.py`

3. **Results Documented**
   - Base Model: 6.61/20 average
   - Fine-Tuned: 6.50/20 average
   - Difference: -0.11 (not statistically significant)
   - 8/10 questions: Identical scores (no improvement)

4. **Infrastructure**
   - All code organized and pushed to GitHub
   - Transfer scripts for server migration
   - GRPO training scripts for RL fine-tuning

---

## What Didn't Work & Why

### ‚ùå Problems Identified

1. **Insufficient Training**
   - **What:** Only 500 steps trained
   - **Why:** Minimal training time, LoRA adapters barely changed
   - **Impact:** Model behavior unchanged
   - **Solution:** Need 10,000+ steps (3-5 epochs)

2. **Dataset Mismatch**
   - **What:** Generic legal text vs. specific legal questions
   - **Why:** Dataset is general legal corpus, not Q&A format
   - **Impact:** Model learned format, not reasoning
   - **Solution:** Use domain-specific Q&A datasets

3. **Low Scores on Both Models**
   - **What:** Base: 6.61/20, Fine-tuned: 6.50/20
   - **Why:** Legal questions are genuinely difficult
   - **Impact:** Even baseline performance is poor
   - **Solution:** Need better base model or RAG approach

4. **Evaluation Issues**
   - **What:** 8/10 questions had identical scores
   - **Why:** Responses too similar, scoring may be too strict
   - **Impact:** Can't distinguish improvements
   - **Solution:** Better evaluation metrics or human evaluation

---

## Technical Details

### Training Configuration

```python
# LoRA Configuration
LoRA Rank (r): 16
LoRA Alpha: 32
Target Modules: ["q_proj", "k_proj", "v_proj", "o_proj"]
Dropout: 0.05

# Training Parameters
Batch Size: 1 per device
Gradient Accumulation: 16 steps
Learning Rate: 2e-4
Max Steps: 500
Warmup Steps: 50
Precision: BF16
```

### Model Location
- **Checkpoint:** `/qwen2.5-32b-law-finetuned/checkpoint-500/`
- **Size:** ~400MB (LoRA adapters only)
- **Base Model:** Not fine-tuned, loaded from Hugging Face cache

### Dataset
- **Name:** `lamblamb/pile_of_law_subset`
- **Type:** Legal text corpus
- **Format:** Raw text, not instruction-following
- **Size:** Unknown (subset of full pile-of-law)

---

## Evaluation Results Breakdown

### By Domain

| Domain | Base Score | Fine-Tuned | Difference |
|--------|------------|------------|------------|
| Contract Law | 7.35 | 7.35 | 0.00 |
| Constitutional Law | 6.88 | 6.88 | 0.00 |
| Corporate Law | 10.73 | 9.31 | **-1.42** |
| Legal Ethics | 9.79 | 10.09 | **+0.30** |
| Criminal Law | 4.35 | 4.35 | 0.00 |
| Tort Law | 6.48 | 6.48 | 0.00 |
| Property Law | 5.27 | 5.27 | 0.00 |
| IP Law | 5.04 | 5.04 | 0.00 |
| International Law | 5.33 | 5.33 | 0.00 |
| Evidence Law | 4.86 | 4.86 | 0.00 |

**Key Insight:** Only Legal Ethics showed improvement (+0.30), Corporate Law got worse (-1.42).

### By Criteria

| Criterion | Base | Fine-Tuned | Difference |
|-----------|------|-------------|------------|
| Knowledge Depth | 1.15/5 | 1.07/5 | -0.08 |
| Accuracy | 2.99/5 | 2.99/5 | 0.00 |
| Reasoning | 0.27/5 | 0.24/5 | -0.03 |
| Completeness | 2.20/5 | 2.20/5 | 0.00 |

**Key Insight:** Reasoning scores extremely low (0.27/5 = 5%), Knowledge Depth also low (1.15/5 = 23%).

---

## Lessons Learned

### ‚úÖ What Worked

1. **Infrastructure Setup**
   - 8x MI300X GPUs handled 32B model well
   - LoRA efficient (only ~400MB adapters)
   - Checkpoint system worked (saved every 50 steps)

2. **Evaluation Framework**
   - Comprehensive evaluation script created
   - Multiple criteria scored
   - Statistical analysis included

3. **Code Organization**
   - Well-structured repository
   - Transfer scripts for migration
   - Documentation maintained

### ‚ùå What Didn't Work

1. **Training Duration**
   - 500 steps = ~30 minutes = insufficient
   - LoRA adapters barely changed
   - Model behavior unchanged

2. **Dataset Selection**
   - Generic legal text ‚â† legal Q&A
   - Format mismatch (text vs. instructions)
   - Domain coverage unclear

3. **Evaluation Limitations**
   - Automated scoring may be too strict
   - Can't distinguish small improvements
   - Need human evaluation for validation

---

## Future Improvement Roadmap

### Priority 1: Increase Training (HIGHEST PRIORITY)

**Action:** Train for 10,000+ steps (3-5 epochs)

**Why:** 500 steps is like 1 hour of studying. Need weeks of training.

**How:**
```python
# Edit: scripts/training/finetune_qwen_law_fixed.py
training_args = TrainingArguments(
    max_steps=10000,  # Instead of 500
    num_train_epochs=3,  # Full epochs
    per_device_train_batch_size=2,  # Increase batch size
    gradient_accumulation_steps=8,  # Reduce accumulation
    # ... rest of config
)
```

**Expected Time:** 2-6 hours (depending on dataset size)
**Expected Improvement:** +2-5 points on evaluation

### Priority 2: Better Dataset (HIGH PRIORITY)

**Action:** Use domain-specific legal Q&A datasets

**Options:**
1. **LegalBench** - Legal reasoning tasks
   ```python
   from datasets import load_dataset
   dataset = load_dataset("nguha/legalbench", "all")
   ```

2. **LEX-GLUE** - Legal NLP benchmarks
   ```python
   dataset = load_dataset("lex_glue", "case_hold")
   ```

3. **Create Custom Dataset**
   - Format as instruction-following
   - Include Q&A pairs matching evaluation questions
   - Cover all 10 legal domains tested

**Why:** Dataset must match evaluation format (Q&A, not raw text)

### Priority 3: Domain-Specific Fine-Tuning (MEDIUM PRIORITY)

**Action:** Train separate models for each legal domain

**Why:** 
- Legal domains are different (contract vs. criminal vs. constitutional)
- One model trying to learn all domains = too hard
- Specialized models perform better

**How:**
```python
# Train separate models
python3 finetune_contract_law.py    # Contract-specific
python3 finetune_criminal_law.py    # Criminal-specific
python3 finetune_constitutional.py   # Constitutional-specific
# etc.
```

### Priority 4: Better Evaluation (MEDIUM PRIORITY)

**Action:** Improve evaluation methodology

**Options:**
1. **Human Evaluation**
   - Have legal experts score responses
   - More accurate than automated scoring

2. **Better Metrics**
   - Use legal-specific benchmarks (LegalBench)
   - Compare against legal expert answers
   - Measure citation accuracy

3. **Ablation Studies**
   - Test different LoRA ranks (r=8, 16, 32, 64)
   - Test different learning rates
   - Test different datasets

### Priority 5: Alternative Approaches (LOW PRIORITY)

**Action:** Try different methods

**Options:**
1. **RAG (Retrieval-Augmented Generation)**
   - Combine model with legal knowledge base
   - Better for specific legal questions
   - No training needed

2. **Domain Adaptation**
   - Fine-tune base model on legal text first
   - Then fine-tune on Q&A format
   - Two-stage training

3. **RLHF (Reinforcement Learning)**
   - Train reward model on legal expert preferences
   - Fine-tune with RLHF
   - Better alignment with legal reasoning

---

## Technical Recommendations

### For Next Training Run

1. **Increase Training Steps**
   ```python
   max_steps=10000  # Minimum
   num_train_epochs=3  # Full epochs
   ```

2. **Optimize Batch Size**
   ```python
   per_device_train_batch_size=2  # Increase if memory allows
   gradient_accumulation_steps=8  # Reduce accumulation
   ```

3. **Use Better Dataset**
   ```python
   # Use LegalBench or create Q&A format
   dataset = load_dataset("nguha/legalbench")
   # Format as instruction-following
   ```

4. **Monitor Training**
   ```python
   logging_steps=10  # More frequent logging
   eval_steps=100  # Add evaluation during training
   save_steps=500  # Save checkpoints regularly
   ```

5. **Experiment with LoRA**
   ```python
   # Try different ranks
   r=32  # Instead of 16 (more parameters)
   lora_alpha=64  # Instead of 32
   ```

---

## Evaluation Questions (Reference)

These are the 10 questions used for evaluation:

1. **Contract Law:** "When does an offer become irrevocable under UCC ¬ß 2-205, and how does this differ from common-law options?"

2. **Constitutional Law:** "Explain the strict scrutiny test and give two cases where it was applied."

3. **Criminal Law:** "Analyze whether an accomplice who withdraws before the crime occurs is still criminally liable."

4. **Tort Law:** "Compare negligence per se and res ipsa loquitur; when does each apply?"

5. **Property Law:** "Discuss the rule against perpetuities and its modern reforms."

6. **Corporate Law:** "When can the corporate veil be pierced? Give examples."

7. **Intellectual Property:** "Differentiate between copyright and trademark fair-use doctrines."

8. **International Law:** "Explain state immunity under the FSIA and its exceptions."

9. **Evidence Law:** "Outline the hearsay rule and three major exceptions."

10. **Legal Ethics:** "Under the ABA Model Rules, when must a lawyer withdraw from representation?"

**Scoring:** 0-5 points per criterion (Knowledge Depth, Accuracy, Reasoning, Completeness) = 20 points max per question.

---

## Files & Locations

### Code Repository
- **GitHub:** https://github.com/Arnie016/Law_Qwen
- **Structure:** Organized into scripts/, docs/, models/, data/

### Model Checkpoints
- **Location:** `models/checkpoints/checkpoint-500/`
- **Size:** ~400MB (LoRA adapters)
- **Base Model:** Qwen/Qwen2.5-32B-Instruct (from Hugging Face)

### Evaluation Results
- **File:** `data/evaluation/legal_eval_results.csv` (if available)
- **Script:** `scripts/evaluation/eval_legal_models_scientific.py`

### Training Scripts
- **Main:** `scripts/training/finetune_qwen_law_fixed.py`
- **Config:** LoRA r=16, alpha=32, max_steps=500

---

## Quick Start for Future Improvement

### Step 1: Increase Training
```bash
cd scripts/training
# Edit finetune_qwen_law_fixed.py: max_steps=10000
python3 finetune_qwen_law_fixed.py
```

### Step 2: Use Better Dataset
```python
# Use LegalBench instead
from datasets import load_dataset
dataset = load_dataset("nguha/legalbench", "all")
# Format as instruction-following
```

### Step 3: Re-Evaluate
```bash
cd scripts/evaluation
python3 eval_legal_models_scientific.py
```

### Step 4: Compare Results
- Compare new scores vs. baseline (6.61/20)
- Target: 10-12/20 (moderate improvement)
- Ideal: 15+/20 (significant improvement)

---

## Success Criteria

### Minimum Success
- **Score:** 8-10/20 average (vs. 6.61 baseline)
- **Improvement:** +1.5-3.5 points
- **Consistency:** Improvement in 6+/10 domains

### Good Success
- **Score:** 10-12/20 average
- **Improvement:** +3.5-5.5 points
- **Consistency:** Improvement in 8+/10 domains

### Excellent Success
- **Score:** 15+/20 average
- **Improvement:** +8+ points
- **Consistency:** Improvement in all domains

---

## Conclusion

**Current Status:** ‚ùå Fine-tuning did not improve legal reasoning.

**Root Cause:** Insufficient training (500 steps) + dataset mismatch.

**Path Forward:** 
1. Train longer (10,000+ steps)
2. Use better dataset (LegalBench or Q&A format)
3. Domain-specific fine-tuning
4. Better evaluation

**Expected Timeline:** 2-6 hours training + evaluation

**Expected Outcome:** Moderate improvement (+2-5 points) with proper training and dataset.

---

## For Future AI/Model

**Context:** This project fine-tuned Qwen 2.5 32B on legal datasets. Results showed no improvement after 500 steps. Evaluation framework exists. Infrastructure ready.

**Your Task:** Improve upon this work by:
1. Increasing training duration to 10,000+ steps
2. Using domain-specific legal Q&A datasets (LegalBench recommended)
3. Experimenting with different LoRA configurations
4. Re-evaluating with same evaluation script
5. Documenting improvements and lessons learned

**Key Files:**
- Training: `scripts/training/finetune_qwen_law_fixed.py`
- Evaluation: `scripts/evaluation/eval_legal_models_scientific.py`
- Results: This document summarizes what was done

**Success Metric:** Achieve 10+/20 average score (vs. 6.61 baseline) with consistent improvement across domains.

Good luck! üöÄ

