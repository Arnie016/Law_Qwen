{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ Video Fine-Tuning & Open-Sora Explained\n",
        "\n",
        "## What is Open-Sora?\n",
        "\n",
        "**Open-Sora** is an open-source video generation model that creates videos from text prompts.\n",
        "\n",
        "**Key Features:**\n",
        "- **Text-to-Video:** Generate videos from text descriptions\n",
        "- **Open Source:** Free, customizable, can be fine-tuned\n",
        "- **High Quality:** Professional video generation\n",
        "- **Flexible:** Can generate various video styles and lengths\n",
        "\n",
        "**Comparison:**\n",
        "- **Stable Video Diffusion:** Image ‚Üí Video (needs image first)\n",
        "- **Open-Sora:** Text ‚Üí Video (direct generation)\n",
        "- **Open-Sora:** More flexible, can generate longer videos\n",
        "\n",
        "**Your 205GB VRAM:** Perfect for Open-Sora! Can load large models and generate long videos.\n",
        "\n",
        "---\n",
        "\n",
        "## Can You Fine-Tune on Video Datasets?\n",
        "\n",
        "**YES!** You can fine-tune models on video datasets for:\n",
        "\n",
        "1. **Video Understanding** (Video ‚Üí Text)\n",
        "   - Video captioning\n",
        "   - Video question answering\n",
        "   - Action recognition\n",
        "\n",
        "2. **Video Generation** (Text ‚Üí Video)\n",
        "   - Style transfer\n",
        "   - Domain-specific videos\n",
        "   - Custom video generation\n",
        "\n",
        "Let's explore both!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Required Libraries\n",
        "# Run this first!\n",
        "\n",
        "print(\"üì¶ Installing video generation libraries...\\n\")\n",
        "\n",
        "# Install diffusers and opencv\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "packages = [\n",
        "    \"diffusers\",\n",
        "    \"opencv-python\",\n",
        "    \"accelerate\",\n",
        "    \"xformers\",  # Optional but helps with speed\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    print(f\"Installing {pkg}...\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "    print(f\"‚úÖ {pkg} installed\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries installed!\")\n",
        "print(\"üöÄ Ready for video generation and fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify Installation\n",
        "import torch\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "\n",
        "print(\"‚úÖ diffusers installed!\")\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ GPU: {torch.cuda.is_available()}\")\n",
        "print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "\n",
        "print(\"\\nüé¨ Ready for video generation!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìπ Video Fine-Tuning Options\n",
        "\n",
        "### Option 1: Video Understanding (Video ‚Üí Text)\n",
        "\n",
        "**What:** Train models to understand video content\n",
        "- **Input:** Video frames\n",
        "- **Output:** Text descriptions, answers, captions\n",
        "- **Models:** Qwen2.5-VL, LLaVA-Video, Video-LLM\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "Input: Video of a cat playing\n",
        "Output: \"A cat is playing with a ball in a living room\"\n",
        "```\n",
        "\n",
        "### Option 2: Video Generation Fine-Tuning (Text ‚Üí Video)\n",
        "\n",
        "**What:** Fine-tune video generation models\n",
        "- **Input:** Text prompts\n",
        "- **Output:** Custom videos\n",
        "- **Models:** Open-Sora, Stable Video Diffusion, AnimateDiff\n",
        "\n",
        "**Example:**\n",
        "```python\n",
        "Input: \"A legal courtroom scene\"\n",
        "Output: Generated video of courtroom\n",
        "```\n",
        "\n",
        "### Option 3: Video Style Transfer\n",
        "\n",
        "**What:** Apply styles to videos\n",
        "- **Input:** Video + style prompt\n",
        "- **Output:** Styled video\n",
        "- **Models:** Stable Diffusion + Video Diffusion\n",
        "\n",
        "Let's try each!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 1: Video Understanding Fine-Tuning Setup\n",
        "# Train a model to understand video content\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "import torch\n",
        "\n",
        "print(\"üé¨ Video Understanding Fine-Tuning Setup\\n\")\n",
        "\n",
        "# Example: Load video dataset\n",
        "print(\"1. Loading video dataset...\")\n",
        "try:\n",
        "    # Try loading a video dataset\n",
        "    dataset = load_dataset(\"lmms-lab/LLaVA-Video-178K\", split=\"train[:10]\")\n",
        "    print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
        "    print(f\"‚úÖ Keys: {dataset[0].keys()}\")\n",
        "    \n",
        "    # Example format\n",
        "    print(\"\\nüìù Example entry:\")\n",
        "    example = dataset[0]\n",
        "    print(f\"   Video: {example.get('video', 'N/A')}\")\n",
        "    print(f\"   Question: {example.get('question', 'N/A')}\")\n",
        "    print(f\"   Answer: {example.get('answer', 'N/A')}\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Dataset not available: {e}\")\n",
        "    print(\"üí° Can use custom video dataset\")\n",
        "\n",
        "print(\"\\n2. Model setup:\")\n",
        "print(\"   - Qwen2.5-VL-72B-Instruct (best for video understanding)\")\n",
        "print(\"   - LLaVA-NeXT-Video (good alternative)\")\n",
        "print(\"   - Video-LLM (smaller, faster)\")\n",
        "\n",
        "print(\"\\n‚úÖ Ready for video understanding fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 2: Video Generation Fine-Tuning (Open-Sora Style)\n",
        "# Fine-tune video generation models\n",
        "\n",
        "print(\"üé¨ Video Generation Fine-Tuning\\n\")\n",
        "\n",
        "print(\"What is Open-Sora?\")\n",
        "print(\"=\" * 50)\n",
        "print(\"\"\"\n",
        "Open-Sora is an open-source video generation model that:\n",
        "\n",
        "1. Generates videos from text prompts (text-to-video)\n",
        "2. Creates high-quality, consistent videos\n",
        "3. Can be fine-tuned on custom datasets\n",
        "4. Supports various video styles and lengths\n",
        "\n",
        "Key Features:\n",
        "- Text ‚Üí Video generation\n",
        "- Long video support (60+ seconds)\n",
        "- High resolution (up to 1080p)\n",
        "- Open source (free to use and modify)\n",
        "\n",
        "Installation:\n",
        "- GitHub: https://github.com/hpcaitech/Open-Sora\n",
        "- May need ROCm-specific setup\n",
        "- Requires significant VRAM (you have 205GB - perfect!)\n",
        "\n",
        "Use Cases:\n",
        "- Creative video generation\n",
        "- Custom video styles\n",
        "- Domain-specific videos (legal, medical, etc.)\n",
        "- Long-form video content\n",
        "\"\"\")\n",
        "\n",
        "print(\"\\nüìù Fine-Tuning Video Generation:\")\n",
        "print(\"   1. Collect video dataset\")\n",
        "print(\"   2. Train on Stable Video Diffusion or Open-Sora\")\n",
        "print(\"   3. Generate custom videos\")\n",
        "\n",
        "print(\"\\nüí° With your VRAM, you can:\")\n",
        "print(\"   - Load large video models\")\n",
        "print(\"   - Generate long videos (50+ frames)\")\n",
        "print(\"   - Fine-tune on large datasets\")\n",
        "print(\"   - Process multiple videos simultaneously\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 3: Quick Video Generation Test\n",
        "# Test video generation with Stable Video Diffusion\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline, StableVideoDiffusionPipeline\n",
        "from PIL import Image\n",
        "import imageio\n",
        "\n",
        "print(\"üé¨ Testing Video Generation\\n\")\n",
        "\n",
        "# Step 1: Generate image\n",
        "print(\"1. Generating base image...\")\n",
        "try:\n",
        "    pipe_img = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe_img = pipe_img.to(\"cuda\")\n",
        "    \n",
        "    prompt = \"a futuristic city at night, neon lights, cyberpunk style\"\n",
        "    print(f\"   Prompt: {prompt}\")\n",
        "    \n",
        "    image = pipe_img(prompt, num_inference_steps=20).images[0]\n",
        "    image.save(\"test_image.png\")\n",
        "    print(\"   ‚úÖ Image generated and saved!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Image generation error: {e}\")\n",
        "    # Create a simple test image\n",
        "    image = Image.new('RGB', (512, 512), color='purple')\n",
        "    print(\"   ‚úÖ Using test image\")\n",
        "\n",
        "# Step 2: Generate video from image\n",
        "print(\"\\n2. Generating video from image...\")\n",
        "try:\n",
        "    pipe_video = StableVideoDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe_video = pipe_video.to(\"cuda\")\n",
        "    \n",
        "    video_frames = pipe_video(\n",
        "        image,\n",
        "        num_frames=14,  # Start with fewer frames for testing\n",
        "        decode_chunk_size=4,\n",
        "    ).frames[0]\n",
        "    \n",
        "    print(f\"   ‚úÖ Generated {len(video_frames)} frames!\")\n",
        "    \n",
        "    # Save video\n",
        "    imageio.mimwrite(\"test_video.mp4\", video_frames, fps=7)\n",
        "    print(\"   ‚úÖ Video saved to test_video.mp4\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Video generation error: {e}\")\n",
        "    print(\"   üí° Install: pip install diffusers imageio\")\n",
        "    print(\"   üí° May need to download model weights first\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Video Fine-Tuning Datasets\n",
        "\n",
        "### For Video Understanding:\n",
        "\n",
        "1. **LLaVA-Video-178K**\n",
        "   - 178K video-text pairs\n",
        "   - Video captioning and QA\n",
        "   - Format: Video + Questions + Answers\n",
        "\n",
        "2. **Video-MME**\n",
        "   - Multiple choice questions\n",
        "   - Video understanding tasks\n",
        "   - Good for evaluation\n",
        "\n",
        "3. **Custom Dataset**\n",
        "   - Your own videos + descriptions\n",
        "   - Legal videos + legal analysis\n",
        "   - Domain-specific content\n",
        "\n",
        "### For Video Generation:\n",
        "\n",
        "1. **Custom Video Collection**\n",
        "   - Collect videos in your style\n",
        "   - Label with text prompts\n",
        "   - Train model to generate similar videos\n",
        "\n",
        "2. **Domain-Specific Videos**\n",
        "   - Legal videos (courtroom, legal education)\n",
        "   - Medical videos (procedures, explanations)\n",
        "   - Educational videos\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. **Install libraries:** Run the install cell above\n",
        "2. **Try video generation:** Test with Stable Video Diffusion\n",
        "3. **Load video dataset:** Start with LLaVA-Video-178K\n",
        "4. **Fine-tune model:** Train on your video dataset\n",
        "\n",
        "**Your 205GB VRAM is perfect for video fine-tuning!** üé¨\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
