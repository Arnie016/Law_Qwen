{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ Creative Video Diffusion\n",
        "\n",
        "With 205GB VRAM free, you can run impressive video generation models!\n",
        "\n",
        "**Available Models:**\n",
        "1. **Stable Video Diffusion** - High-quality video generation\n",
        "2. **ModelScope** - Video generation from text/prompts\n",
        "3. **Open-Sora** - Open-source video generation\n",
        "4. **Video-LLM** - Text-to-video with language models\n",
        "\n",
        "**Note:** Some models may need specific setup. Let's try the most compatible ones first!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup: Check for video generation libraries\n",
        "import torch\n",
        "import sys\n",
        "\n",
        "print(\"üîç Checking video generation capabilities...\\n\")\n",
        "\n",
        "# Check PyTorch\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ GPU: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\\n\")\n",
        "\n",
        "# Check available libraries\n",
        "video_libs = {\n",
        "    'diffusers': 'Hugging Face diffusion models',\n",
        "    'transformers': 'Video transformers',\n",
        "    'torchvision': 'Video processing',\n",
        "    'imageio': 'Video I/O',\n",
        "    'opencv-python': 'cv2 - video processing',\n",
        "}\n",
        "\n",
        "print(\"üì¶ Checking libraries:\")\n",
        "for lib, desc in video_libs.items():\n",
        "    try:\n",
        "        mod = __import__(lib)\n",
        "        v = getattr(mod, '__version__', '?')\n",
        "        print(f\"   ‚úÖ {lib}: {v}\")\n",
        "    except:\n",
        "        print(f\"   ‚ùå {lib}: NOT INSTALLED ({desc})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Install Video Generation Libraries\n",
        "# Run this if libraries are missing\n",
        "\n",
        "print(\"üì¶ Installing video generation libraries...\")\n",
        "\n",
        "# Uncomment to install:\n",
        "# !pip install diffusers transformers accelerate imageio opencv-python\n",
        "\n",
        "print(\"‚úÖ Libraries installed (or already available)\")\n",
        "print(\"\\nüìù Available video models:\")\n",
        "print(\"   - stabilityai/stable-video-diffusion-img2vid\")\n",
        "print(\"   - modelscope/text-to-video-synthesis\")\n",
        "print(\"   - THUDM/CogVideoX\")\n",
        "print(\"   - open-sora (from Open-Sora project)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 1: Stable Video Diffusion (Image-to-Video)\n",
        "# Generate video from a single image\n",
        "\n",
        "try:\n",
        "    from diffusers import StableVideoDiffusionPipeline\n",
        "    from diffusers.utils import load_image\n",
        "    import torch\n",
        "    \n",
        "    print(\"üé¨ Loading Stable Video Diffusion...\")\n",
        "    \n",
        "    # Load pipeline\n",
        "    pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-video-diffusion-img2vid\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    \n",
        "    # Load an image (you can provide your own)\n",
        "    print(\"üì∏ Loading image...\")\n",
        "    # image = load_image(\"https://example.com/your-image.jpg\")\n",
        "    # Or create a simple test image\n",
        "    from PIL import Image\n",
        "    import numpy as np\n",
        "    \n",
        "    # Create a test image (replace with your image)\n",
        "    test_image = Image.new('RGB', (512, 512), color='red')\n",
        "    \n",
        "    # Generate video\n",
        "    print(\"üé• Generating video from image...\")\n",
        "    frames = pipe(\n",
        "        image=test_image,\n",
        "        decode_chunk_size=8,\n",
        "        generator=torch.manual_seed(42),\n",
        "    ).frames[0]\n",
        "    \n",
        "    print(f\"‚úÖ Generated {len(frames)} frames!\")\n",
        "    \n",
        "    # Save video\n",
        "    import imageio\n",
        "    imageio.mimsave(\"generated_video.mp4\", frames, fps=7)\n",
        "    print(\"üíæ Saved to generated_video.mp4\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "    print(\"üí° Try installing: pip install diffusers imageio\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 2: Text-to-Video with Transformers\n",
        "# Generate video from text description\n",
        "\n",
        "try:\n",
        "    from transformers import AutoModelForCausalLM, AutoProcessor\n",
        "    import torch\n",
        "    \n",
        "    print(\"üé¨ Loading Video Generation Model...\")\n",
        "    \n",
        "    # Try CogVideoX or similar model\n",
        "    # Note: Some models may not be available - adjust as needed\n",
        "    model_name = \"THUDM/CogVideoX-17B\"  # Or try other video models\n",
        "    \n",
        "    processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True,\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Model loaded!\")\n",
        "    \n",
        "    # Generate video from text\n",
        "    prompt = \"A cat walking on the street\"\n",
        "    print(f\"üìù Prompt: {prompt}\")\n",
        "    print(\"üé• Generating video...\")\n",
        "    \n",
        "    inputs = processor(text=prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        video = model.generate(**inputs, max_length=100)\n",
        "    \n",
        "    print(\"‚úÖ Video generated!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "    print(\"üí° This model may not be available or may need different setup\")\n",
        "    print(\"üí° Try Stable Video Diffusion (image-to-video) instead\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 3: Video Generation with Open-Sora (If Available)\n",
        "# Open-source video generation\n",
        "\n",
        "try:\n",
        "    import sys\n",
        "    sys.path.append('/path/to/open-sora')  # Adjust path if needed\n",
        "    \n",
        "    from opensora import OpenSora\n",
        "    \n",
        "    print(\"üé¨ Loading Open-Sora...\")\n",
        "    \n",
        "    # Initialize Open-Sora\n",
        "    opensora = OpenSora(\n",
        "        model_path=\"Open-Sora/Open-Sora\",\n",
        "        device=\"cuda\",\n",
        "    )\n",
        "    \n",
        "    # Generate video\n",
        "    prompt = \"A beautiful sunset over the ocean\"\n",
        "    print(f\"üìù Prompt: {prompt}\")\n",
        "    print(\"üé• Generating video...\")\n",
        "    \n",
        "    video = opensora.generate(\n",
        "        prompt=prompt,\n",
        "        num_frames=16,\n",
        "        height=512,\n",
        "        width=512,\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Video generated!\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Open-Sora not available: {e}\")\n",
        "    print(\"üí° Install from: https://github.com/hpcaitech/Open-Sora\")\n",
        "    print(\"üí° Or use Stable Video Diffusion instead\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 4: Video Editing/Style Transfer\n",
        "# Apply styles to video frames\n",
        "\n",
        "try:\n",
        "    from diffusers import StableDiffusionImg2ImgPipeline\n",
        "    import torch\n",
        "    from PIL import Image\n",
        "    import imageio\n",
        "    \n",
        "    print(\"üé® Loading Style Transfer Model...\")\n",
        "    \n",
        "    pipe = StableDiffusionImg2ImgPipeline.from_pretrained(\n",
        "        \"runwayml/stable-diffusion-v1-5\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe = pipe.to(\"cuda\")\n",
        "    \n",
        "    # Load or create video frames\n",
        "    print(\"üìπ Processing video frames...\")\n",
        "    \n",
        "    # Example: Process a single frame\n",
        "    # For full video, loop through frames\n",
        "    prompt = \"anime style, colorful, detailed\"\n",
        "    init_image = Image.new('RGB', (512, 512), color='blue')\n",
        "    \n",
        "    image = pipe(\n",
        "        prompt=prompt,\n",
        "        image=init_image,\n",
        "        strength=0.75,\n",
        "    ).images[0]\n",
        "    \n",
        "    print(\"‚úÖ Frame styled!\")\n",
        "    print(\"üí° For full video: loop through all frames\")\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Error: {e}\")\n",
        "    print(\"üí° Install: pip install diffusers\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Best Video Generation Options for Your Setup\n",
        "\n",
        "### ‚úÖ Recommended (Most Compatible):\n",
        "\n",
        "1. **Stable Video Diffusion** (Image-to-Video)\n",
        "   - Most stable and compatible\n",
        "   - Works with ROCm\n",
        "   - High quality output\n",
        "   - Install: `pip install diffusers imageio`\n",
        "\n",
        "2. **Video Diffusion Models** (Hugging Face)\n",
        "   - Various models available\n",
        "   - Text-to-video options\n",
        "   - Check compatibility per model\n",
        "\n",
        "### ‚ö†Ô∏è May Need Setup:\n",
        "\n",
        "3. **Open-Sora**\n",
        "   - Requires manual installation\n",
        "   - May need ROCm-specific builds\n",
        "   - Very powerful if working\n",
        "\n",
        "4. **ModelScope**\n",
        "   - Chinese video models\n",
        "   - May need VPN/access\n",
        "\n",
        "### üí° Quick Start:\n",
        "\n",
        "**Easiest: Stable Video Diffusion**\n",
        "```python\n",
        "from diffusers import StableVideoDiffusionPipeline\n",
        "pipe = StableVideoDiffusionPipeline.from_pretrained(\n",
        "    \"stabilityai/stable-video-diffusion-img2vid\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        ").to(\"cuda\")\n",
        "```\n",
        "\n",
        "**With your 205GB VRAM:**\n",
        "- Can load largest video models\n",
        "- Generate high-resolution videos\n",
        "- Process multiple videos simultaneously\n",
        "- Long videos (many frames)\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. **Install libraries:** `pip install diffusers imageio opencv-python`\n",
        "2. **Try Stable Video Diffusion** (most compatible)\n",
        "3. **Generate from images** you create/upload\n",
        "4. **Experiment with prompts** for creative videos\n",
        "\n",
        "**Your GPU is POWERFUL - perfect for video generation!** üé¨\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
