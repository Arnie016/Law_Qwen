{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Quick Test - Check Your Environment\n",
        "\n",
        "Run the cells below to test what's available in your session.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 1: Python Version\n",
        "import sys\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Python Path: {sys.executable}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 2: Check GPU\n",
        "import torch\n",
        "print(f\"✅ PyTorch: {torch.__version__}\")\n",
        "print(f\"✅ GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"✅ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"✅ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "    print(f\"✅ ROCm Version: {torch.version.hip}\")\n",
        "else:\n",
        "    print(\"❌ GPU not detected\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 3: Check Installed Packages\n",
        "import importlib\n",
        "\n",
        "packages = [\n",
        "    'torch',\n",
        "    'transformers',\n",
        "    'datasets',\n",
        "    'peft',\n",
        "    'unsloth',\n",
        "    'trl',\n",
        "]\n",
        "\n",
        "print(\"Checking installed packages:\")\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        mod = importlib.import_module(pkg)\n",
        "        version = getattr(mod, '__version__', 'unknown')\n",
        "        print(f\"✅ {pkg}: {version}\")\n",
        "    except ImportError:\n",
        "        print(f\"❌ {pkg}: NOT INSTALLED\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 4: Simple GPU Test\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    # Create a simple tensor on GPU\n",
        "    x = torch.randn(1000, 1000).cuda()\n",
        "    y = torch.randn(1000, 1000).cuda()\n",
        "    z = torch.matmul(x, y)\n",
        "    print(f\"✅ GPU computation works!\")\n",
        "    print(f\"✅ Result shape: {z.shape}\")\n",
        "    print(f\"✅ Result device: {z.device}\")\n",
        "else:\n",
        "    print(\"❌ GPU not available for testing\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 5: Check Directories\n",
        "import os\n",
        "\n",
        "print(\"Current directory:\", os.getcwd())\n",
        "print(\"\\nContents:\")\n",
        "for item in os.listdir('.'):\n",
        "    print(f\"  - {item}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 6: Load a Small Model (Quick Test)\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "print(\"Testing tokenizer loading...\")\n",
        "try:\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-32B-Instruct\", trust_remote_code=True)\n",
        "    print(\"✅ Tokenizer loaded!\")\n",
        "    \n",
        "    # Test encoding\n",
        "    test_text = \"What is negligence?\"\n",
        "    tokens = tokenizer(test_text, return_tensors=\"pt\")\n",
        "    print(f\"✅ Encoded text: {len(tokens['input_ids'][0])} tokens\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test 7: Check Dataset Loading\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"Testing dataset loading...\")\n",
        "try:\n",
        "    # Try loading a small dataset\n",
        "    dataset = load_dataset(\"lamblamb/pile_of_law_subset\", split=\"train[:10]\")\n",
        "    print(f\"✅ Dataset loaded!\")\n",
        "    print(f\"✅ Examples: {len(dataset)}\")\n",
        "    print(f\"✅ Keys: {dataset[0].keys()}\")\n",
        "    print(f\"✅ Sample text length: {len(dataset[0]['text'])} chars\")\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ✅ All Tests Complete!\n",
        "\n",
        "If all tests passed, your environment is ready for:\n",
        "- Loading models\n",
        "- Training fine-tuning\n",
        "- Running GRPO\n",
        "\n",
        "If any tests failed, check:\n",
        "- GPU availability (Docker container)\n",
        "- Package installation\n",
        "- Dataset access\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
