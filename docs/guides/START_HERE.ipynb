{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Legal LLM Fine-Tuning - Quick Reference\n",
        "\n",
        "## üö® Critical Rules\n",
        "\n",
        "### Rule 1: Reasoning ‚â† Legal Knowledge\n",
        "- Strong reasoning models don't automatically have legal knowledge\n",
        "- Need BOTH: legal facts + legal reasoning\n",
        "\n",
        "### Rule 2: Legal Knowledge Must Come First\n",
        "1. SFT on legal datasets (10K+ steps) ‚Üí Build knowledge\n",
        "2. GRPO/RL ‚Üí Improve reasoning quality\n",
        "\n",
        "### Rule 3: Use Full Datasets\n",
        "- `pile-of-law/pile-of-law` (full) > `lamblamb/pile_of_law_subset`\n",
        "- Full Pile of Law = millions of examples\n",
        "\n",
        "---\n",
        "\n",
        "## üìä Your Setup\n",
        "\n",
        "- **Model:** Qwen 2.5 32B Instruct\n",
        "- **GPU:** MI300X 192GB\n",
        "- **Training:** LoRA fine-tuning\n",
        "- **Repository:** https://github.com/Arnie016/Law_Qwen\n",
        "\n",
        "---\n",
        "\n",
        "## üîß Quick Commands\n",
        "\n",
        "### Check GPU\n",
        "```python\n",
        "import torch\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìö Documentation\n",
        "\n",
        "- **Rules:** `docs/RULES.md`\n",
        "- **Jupyter Guide:** `docs/guides/JUPYTER_NOTEBOOK_GUIDE.md`\n",
        "- **Training Scripts:** `scripts/training/`\n",
        "- **Evaluation:** `scripts/evaluation/`\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è Common Mistakes\n",
        "\n",
        "1. Using reasoning models expecting legal knowledge\n",
        "2. Running scripts on host instead of Docker container\n",
        "3. Using bitsandbytes on ROCm (disable it)\n",
        "4. Training only 500 steps (need 10K+)\n",
        "5. Using subset dataset instead of full dataset\n",
        "\n",
        "---\n",
        "\n",
        "**See `docs/RULES.md` for complete rules and guidelines.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick GPU Check\n",
        "import torch\n",
        "\n",
        "print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
        "print(f\"‚úÖ GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"‚úÖ Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ùå GPU not detected - check Docker container\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Example: Load and Test Model\n",
        "\n",
        "Run the cell below to load and test the Qwen 2.5 32B model:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Qwen 2.5 32B Model\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-32B-Instruct\"\n",
        "\n",
        "print(f\"Loading {model_name}...\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "print(\"‚úÖ Model loaded!\")\n",
        "\n",
        "# Test prompt\n",
        "prompt = \"What is negligence?\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_length=200)\n",
        "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(f\"\\nResponse: {response}\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
