{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83c\udfac Text-to-Video RL Fine-Tuning (GRPO Only)\n",
    "\n",
    "**Goal:** RL fine-tuning of text-to-video model\n",
    "\n",
    "**Model:** `ali-vilab/text-to-video-ms-1.7b` \u2705 Working\n",
    "**Dataset:** `Rapidata/text-2-video-human-preferences` \u2705 Loaded\n",
    "**Method:** GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "**Focus:** RL fine-tuning ONLY - no extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Model\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"\ud83c\udfac Loading ModelScope Text-to-Video Model...\\n\")\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"ali-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "print(f\"\u2705 Model on {pipe.device}\")\n",
    "print(f\"\u2705 VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Test\n",
    "test_video = pipe(\"A cat walking\", num_inference_steps=25).frames[0]\n",
    "print(f\"\u2705 Test: {len(test_video)} frames generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Human Preference Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"\ud83d\udcf9 Loading Human Preference Dataset...\\n\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"Rapidata/text-2-video-human-preferences\",\n",
    "    split=\"train[:1000]\"\n",
    ")\n",
    "\n",
    "print(f\"\u2705 Dataset: {len(dataset)} examples\")\n",
    "print(f\"\u2705 Keys: {dataset[0].keys()}\")\n",
    "\n",
    "# Show example\n",
    "ex = dataset[0]\n",
    "print(f\"\\n\ud83d\udcdd Example:\")\n",
    "for key in list(ex.keys())[:5]:\n",
    "    val = ex[key]\n",
    "    if isinstance(val, str):\n",
    "        print(f\"   {key}: {val[:80]}...\")\n",
    "    else:\n",
    "        print(f\"   {key}: {type(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Video Quality Reward Function\n",
    "# Scores generated videos for GRPO\n",
    "\n",
    "def video_quality_reward(*args, **kwargs):\n",
    "    \"\"\"Reward function for video generation quality\"\"\"\n",
    "    prompts = kwargs.get('prompts') or kwargs.get('inputs') or (args[0] if args else [])\n",
    "    videos = kwargs.get('responses') or kwargs.get('completions') or (args[1] if len(args) > 1 else [])\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for prompt, video in zip(prompts, videos):\n",
    "        reward = 0.0\n",
    "        \n",
    "        if isinstance(video, list) and len(video) > 0:\n",
    "            num_frames = len(video)\n",
    "            \n",
    "            # Reward frame count\n",
    "            if num_frames >= 14:\n",
    "                reward += 3.0\n",
    "            elif num_frames >= 7:\n",
    "                reward += 1.5\n",
    "            elif num_frames >= 3:\n",
    "                reward += 0.5\n",
    "            else:\n",
    "                reward -= 1.0\n",
    "            \n",
    "            # Reward consistency\n",
    "            if num_frames > 1:\n",
    "                try:\n",
    "                    sizes = [f.size for f in video if hasattr(f, 'size')]\n",
    "                    if sizes:\n",
    "                        diffs = [abs(sizes[i][0] - sizes[i+1][0]) for i in range(len(sizes)-1)]\n",
    "                        avg_diff = sum(diffs) / len(diffs) if diffs else 0\n",
    "                        if avg_diff < 5:\n",
    "                            reward += 2.0\n",
    "                        elif avg_diff < 10:\n",
    "                            reward += 1.0\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            reward += 1.0  # Base reward\n",
    "        \n",
    "        reward = max(-5.0, min(10.0, reward))\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"\u2705 Video reward function created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Format Dataset for GRPO\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_grpo_video(examples):\n",
    "    \"\"\"Format for GRPO training\"\"\"\n",
    "    prompts = []\n",
    "    for prompt in examples.get('prompt', examples.get('text', [])):\n",
    "        formatted = f\"Generate a video: {prompt}\"\n",
    "        prompts.append(formatted)\n",
    "    return {\"prompt\": prompts}\n",
    "\n",
    "grpo_dataset = dataset.map(format_grpo_video, batched=True)\n",
    "\n",
    "print(f\"\u2705 GRPO dataset: {len(grpo_dataset)} examples\")\n",
    "print(f\"\u2705 Example: {grpo_dataset[0]['prompt'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: GRPO Configuration\n",
    "from trl import GRPOConfig\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"./text-to-video-grpo\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=500,\n",
    "    warmup_steps=50,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    num_generations=4,  # Generate 4 videos per prompt\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "print(\"\u2705 GRPO Config:\")\n",
    "print(f\"   Batch: {grpo_config.per_device_train_batch_size}\")\n",
    "print(f\"   Generations: {grpo_config.num_generations}\")\n",
    "print(f\"   Steps: {grpo_config.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: RL Fine-Tuning Setup\n",
    "print(\"\ud83d\ude80 RL Fine-Tuning Setup\\n\")\n",
    "\n",
    "print(\"\u26a0\ufe0f Challenge: Standard GRPOTrainer expects text, not video\")\n",
    "print(\"\ud83d\udca1 Solution: Custom training loop needed\\n\")\n",
    "\n",
    "print(\"Custom Video RL Training Loop:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "For each prompt:\n",
    "  1. Generate 4 videos (using pipe)\n",
    "  2. Score each video (reward function)\n",
    "  3. Rank videos by reward\n",
    "  4. Update model to prefer high-reward videos\n",
    "\n",
    "Implementation: Custom trainer class\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\u2705 Reward function ready!\")\n",
    "print(\"\u2705 Dataset formatted!\")\n",
    "print(\"\u2705 Config ready!\")\n",
    "print(\"\\n\ud83d\udca1 Next: Implement custom video RL trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \u2705 RL Fine-Tuning Ready!\n",
    "\n",
    "**Setup Complete:**\n",
    "- \u2705 ModelScope model loaded\n",
    "- \u2705 Human preference dataset loaded\n",
    "- \u2705 Video reward function created\n",
    "- \u2705 GRPO config ready\n",
    "\n",
    "**Next:** Implement custom video RL training loop\n",
    "\n",
    "**205GB VRAM:** Perfect for video RL! \ud83d\ude80"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}