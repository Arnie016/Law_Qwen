{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üé¨ Text-to-Video Fine-Tuning with RL\n",
        "\n",
        "## Complete Setup Using Hugging Face Models & Datasets\n",
        "\n",
        "**Goal:** Fine-tune text-to-video generation model with RL (GRPO/DPO)\n",
        "\n",
        "**Your Setup:**\n",
        "- ‚úÖ 205GB VRAM - Perfect for large video models\n",
        "- ‚úÖ Unsloth available - Fast training\n",
        "- ‚úÖ ROCm GPU - AMD optimized\n",
        "\n",
        "**Models Available:**\n",
        "- Stable Video Diffusion (Image ‚Üí Video)\n",
        "- AnimateDiff (Text ‚Üí Video)\n",
        "- ModelScope Video (Text ‚Üí Video)\n",
        "\n",
        "**Datasets Available:**\n",
        "- WebVid-2M (video + captions)\n",
        "- MSR-VTT (video + descriptions)\n",
        "- Custom video datasets\n",
        "\n",
        "Let's build a complete fine-tuning pipeline!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Install Dependencies\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "print(\"üì¶ Installing video generation libraries...\\n\")\n",
        "\n",
        "packages = [\n",
        "    \"diffusers\",\n",
        "    \"transformers\",\n",
        "    \"accelerate\",\n",
        "    \"peft\",\n",
        "    \"trl\",\n",
        "    \"imageio\",\n",
        "    \"opencv-python\",\n",
        "    \"pillow\",\n",
        "]\n",
        "\n",
        "for pkg in packages:\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "        print(f\"‚úÖ {pkg}: Already installed\")\n",
        "    except:\n",
        "        print(f\"üì¶ Installing {pkg}...\")\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", pkg, \"-q\"])\n",
        "        print(f\"‚úÖ {pkg} installed\")\n",
        "\n",
        "# Install Unsloth\n",
        "try:\n",
        "    import unsloth\n",
        "    print(\"‚úÖ unsloth: Already installed\")\n",
        "except:\n",
        "    print(\"üì¶ Installing unsloth...\")\n",
        "    subprocess.check_call([\n",
        "        sys.executable, \"-m\", \"pip\", \"install\", \n",
        "        \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\", \"-q\"\n",
        "    ])\n",
        "    print(\"‚úÖ unsloth installed\")\n",
        "\n",
        "print(\"\\n‚úÖ All libraries ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Find Text-to-Video Models on Hugging Face\n",
        "from huggingface_hub import list_models\n",
        "\n",
        "print(\"üîç Searching Hugging Face for text-to-video models...\\n\")\n",
        "\n",
        "# Search for video diffusion models\n",
        "video_models = []\n",
        "\n",
        "try:\n",
        "    models = list_models(\n",
        "        search=\"text-to-video\",\n",
        "        sort=\"downloads\",\n",
        "        direction=-1,\n",
        "        limit=10\n",
        "    )\n",
        "    \n",
        "    print(\"Top Text-to-Video Models:\")\n",
        "    for i, model in enumerate(models, 1):\n",
        "        print(f\"\\n{i}. {model.id}\")\n",
        "        print(f\"   Downloads: {model.downloads:,}\")\n",
        "        print(f\"   Likes: {model.likes}\")\n",
        "        video_models.append(model.id)\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Search error: {e}\")\n",
        "    print(\"\\nüí° Manual list:\")\n",
        "    print(\"   - stabilityai/stable-video-diffusion-img2vid-xt\")\n",
        "    print(\"   - guoyww/animatediff-motion-adapter-v1-5-2\")\n",
        "    print(\"   - damo-vilab/text-to-video-ms-1.7b\")\n",
        "    print(\"   - THUDM/CogVideoX-17B\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Find Video Datasets on Hugging Face\n",
        "from huggingface_hub import list_datasets\n",
        "\n",
        "print(\"üîç Searching Hugging Face for video datasets...\\n\")\n",
        "\n",
        "try:\n",
        "    datasets = list_datasets(\n",
        "        search=\"video text\",\n",
        "        sort=\"downloads\",\n",
        "        direction=-1,\n",
        "        limit=10\n",
        "    )\n",
        "    \n",
        "    print(\"Top Video-Text Datasets:\")\n",
        "    for i, ds in enumerate(datasets, 1):\n",
        "        print(f\"\\n{i}. {ds.id}\")\n",
        "        print(f\"   Downloads: {ds.downloads:,}\")\n",
        "        print(f\"   Likes: {ds.likes}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Search error: {e}\")\n",
        "    print(\"\\nüí° Known datasets:\")\n",
        "    print(\"   - mrm8488/webvid-2M-subset (2M video-text pairs)\")\n",
        "    print(\"   - jameseese/msr-vtt (10K videos)\")\n",
        "    print(\"   - lmms-lab/LLaVA-Video-178K (178K pairs)\")\n",
        "    print(\"   - ActivityNet/ActivityNetCaptions (20K videos)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Load Video Dataset\n",
        "from datasets import load_dataset\n",
        "\n",
        "print(\"üìπ Loading video dataset...\\n\")\n",
        "\n",
        "# Try WebVid subset (smaller, faster)\n",
        "try:\n",
        "    dataset = load_dataset(\"mrm8488/webvid-2M-subset\", split=\"train[:100]\")\n",
        "    print(f\"‚úÖ Dataset loaded: {len(dataset)} examples\")\n",
        "    print(f\"‚úÖ Keys: {dataset[0].keys()}\")\n",
        "    \n",
        "    # Show example\n",
        "    example = dataset[0]\n",
        "    print(f\"\\nüìù Example:\")\n",
        "    print(f\"   Keys: {list(example.keys())}\")\n",
        "    if 'text' in example:\n",
        "        print(f\"   Text: {example['text'][:100]}...\")\n",
        "    if 'video' in example:\n",
        "        print(f\"   Video: {type(example['video'])}\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Dataset error: {e}\")\n",
        "    print(\"\\nüí° Alternative: Create custom dataset\")\n",
        "    print(\"   Format: {'prompt': [...], 'video_path': [...]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 5: Setup Text-to-Video Model\n",
        "import unsloth  # IMPORT FIRST!\n",
        "import torch\n",
        "from diffusers import StableVideoDiffusionPipeline, StableDiffusionPipeline\n",
        "from PIL import Image\n",
        "\n",
        "print(\"üé¨ Loading text-to-video models...\\n\")\n",
        "\n",
        "# Model 1: Image generator (for image-to-video pipeline)\n",
        "print(\"1. Loading Stable Diffusion XL (image generator)...\")\n",
        "try:\n",
        "    pipe_img = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-xl-base-1.0\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe_img = pipe_img.to(\"cuda\")\n",
        "    print(\"   ‚úÖ Image generator loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Error: {e}\")\n",
        "    pipe_img = None\n",
        "\n",
        "# Model 2: Video generator (image ‚Üí video)\n",
        "print(\"\\n2. Loading Stable Video Diffusion...\")\n",
        "try:\n",
        "    pipe_video = StableVideoDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-video-diffusion-img2vid-xt\",\n",
        "        torch_dtype=torch.bfloat16,\n",
        "    )\n",
        "    pipe_video = pipe_video.to(\"cuda\")\n",
        "    print(\"   ‚úÖ Video generator loaded!\")\n",
        "except Exception as e:\n",
        "    print(f\"   ‚ö†Ô∏è Error: {e}\")\n",
        "    print(\"   üí° May need to download model weights first\")\n",
        "    pipe_video = None\n",
        "\n",
        "print(\"\\n‚úÖ Models ready for fine-tuning!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 6: Test Video Generation\n",
        "import imageio\n",
        "\n",
        "if pipe_img and pipe_video:\n",
        "    print(\"üé¨ Testing text-to-video generation...\\n\")\n",
        "    \n",
        "    # Step 1: Generate image from text\n",
        "    prompt = \"a futuristic city at night, neon lights, cyberpunk style\"\n",
        "    print(f\"üìù Prompt: {prompt}\")\n",
        "    print(\"üé® Generating image...\")\n",
        "    \n",
        "    image = pipe_img(prompt, num_inference_steps=20).images[0]\n",
        "    image.save(\"test_base_image.png\")\n",
        "    print(\"   ‚úÖ Image generated!\")\n",
        "    \n",
        "    # Step 2: Generate video from image\n",
        "    print(\"\\nüé• Generating video from image...\")\n",
        "    video_frames = pipe_video(\n",
        "        image,\n",
        "        num_frames=14,\n",
        "        decode_chunk_size=4,\n",
        "    ).frames[0]\n",
        "    \n",
        "    print(f\"   ‚úÖ Generated {len(video_frames)} frames!\")\n",
        "    \n",
        "    # Save video\n",
        "    imageio.mimwrite(\"test_video.mp4\", video_frames, fps=7)\n",
        "    print(\"   ‚úÖ Video saved to test_video.mp4\")\n",
        "    \n",
        "    print(\"\\nüéâ Text-to-video pipeline working!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è Models not loaded. Install diffusers first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ RL Fine-Tuning for Video Generation\n",
        "\n",
        "### Challenge: Video RL Fine-Tuning\n",
        "\n",
        "**Problem:** Standard GRPO/DPO trainers expect text outputs, not video frames.\n",
        "\n",
        "**Solutions:**\n",
        "\n",
        "1. **Two-Stage Approach** (Recommended)\n",
        "   - Stage 1: SFT on video datasets (standard fine-tuning)\n",
        "   - Stage 2: RL on video quality metrics (custom rewards)\n",
        "\n",
        "2. **Video-to-Text Model** (Easier)\n",
        "   - Fine-tune video understanding model (Qwen2.5-VL)\n",
        "   - Use RL on text outputs\n",
        "   - Generate videos separately\n",
        "\n",
        "3. **Custom Video RL Trainer** (Advanced)\n",
        "   - Modify GRPOTrainer for video outputs\n",
        "   - Use video quality metrics (SSIM, PSNR, CLIP score)\n",
        "   - Requires custom implementation\n",
        "\n",
        "### Next Steps:\n",
        "\n",
        "1. **Collect Video Dataset**\n",
        "   - Text prompts + videos\n",
        "   - Format: `{\"prompt\": \"...\", \"video_path\": \"...\"}`\n",
        "\n",
        "2. **Fine-Tune Generation** (SFT)\n",
        "   - Train Stable Video Diffusion on your dataset\n",
        "   - Use standard diffusion training\n",
        "\n",
        "3. **Add RL** (Advanced)\n",
        "   - Custom reward function for video quality\n",
        "   - Modify GRPO trainer for video outputs\n",
        "\n",
        "**Your 205GB VRAM:** Perfect for this! üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
