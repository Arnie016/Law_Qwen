{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üé¨ Text-to-Video RL Fine-Tuning (GRPO Only)\n",
    "\n",
    "**Goal:** RL fine-tuning of text-to-video model\n",
    "\n",
    "**Model:** `ali-vilab/text-to-video-ms-1.7b` ‚úÖ Working\n",
    "**Dataset:** `Rapidata/text-2-video-human-preferences` ‚úÖ Loaded\n",
    "**Method:** GRPO (Group Relative Policy Optimization)\n",
    "\n",
    "**Focus:** RL fine-tuning ONLY - no extra stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load Model\n",
    "import torch\n",
    "from diffusers import DiffusionPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"üé¨ Loading ModelScope Text-to-Video Model...\\n\")\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"ali-vilab/text-to-video-ms-1.7b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "print(f\"‚úÖ Model on {pipe.device}\")\n",
    "print(f\"‚úÖ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "# Test\n",
    "test_video = pipe(\"A cat walking\", num_inference_steps=25).frames[0]\n",
    "print(f\"‚úÖ Test: {len(test_video)} frames generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load Human Preference Dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "print(\"üìπ Loading Human Preference Dataset...\\n\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"Rapidata/text-2-video-human-preferences\",\n",
    "    split=\"train[:1000]\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset: {len(dataset)} examples\")\n",
    "print(f\"‚úÖ Keys: {dataset[0].keys()}\")\n",
    "\n",
    "# Show example\n",
    "ex = dataset[0]\n",
    "print(f\"\\nüìù Example:\")\n",
    "for key in list(ex.keys())[:5]:\n",
    "    val = ex[key]\n",
    "    if isinstance(val, str):\n",
    "        print(f\"   {key}: {val[:80]}...\")\n",
    "    else:\n",
    "        print(f\"   {key}: {type(val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Video Quality Reward Function (FIXED)\n",
    "# Scores generated videos for GRPO\n",
    "\n",
    "def video_quality_reward(*args, **kwargs):\n",
    "    \"\"\"Reward function for video generation quality\"\"\"\n",
    "    prompts = kwargs.get('prompts') or kwargs.get('inputs') or (args[0] if args else [])\n",
    "    videos = kwargs.get('responses') or kwargs.get('completions') or (args[1] if len(args) > 1 else [])\n",
    "    \n",
    "    # Debug: Check what we're getting\n",
    "    if len(videos) == 0:\n",
    "        print(\"‚ö†Ô∏è WARNING: No videos received in reward function!\")\n",
    "        return [0.0] * len(prompts) if prompts else [0.0]\n",
    "    \n",
    "    rewards = []\n",
    "    \n",
    "    for i, (prompt, video) in enumerate(zip(prompts, videos)):\n",
    "        reward = 0.0\n",
    "        \n",
    "        # Debug first video\n",
    "        if i == 0:\n",
    "            print(f\"üìπ Debug: Video type={type(video)}, is_list={isinstance(video, list)}\")\n",
    "            if isinstance(video, list):\n",
    "                print(f\"   Length: {len(video)}\")\n",
    "                if len(video) > 0:\n",
    "                    print(f\"   First frame type: {type(video[0])}\")\n",
    "        \n",
    "        # Check if video is a list of frames\n",
    "        if isinstance(video, list) and len(video) > 0:\n",
    "            num_frames = len(video)\n",
    "            \n",
    "            # Reward frame count\n",
    "            if num_frames >= 14:\n",
    "                reward += 3.0\n",
    "            elif num_frames >= 7:\n",
    "                reward += 1.5\n",
    "            elif num_frames >= 3:\n",
    "                reward += 0.5\n",
    "            else:\n",
    "                reward -= 1.0\n",
    "            \n",
    "            # Reward consistency\n",
    "            if num_frames > 1:\n",
    "                try:\n",
    "                    # Check if frames are PIL Images\n",
    "                    if hasattr(video[0], 'size'):\n",
    "                        sizes = [f.size for f in video if hasattr(f, 'size')]\n",
    "                        if sizes:\n",
    "                            diffs = [abs(sizes[i][0] - sizes[i+1][0]) for i in range(len(sizes)-1)]\n",
    "                            avg_diff = sum(diffs) / len(diffs) if diffs else 0\n",
    "                            if avg_diff < 5:\n",
    "                                reward += 2.0\n",
    "                            elif avg_diff < 10:\n",
    "                                reward += 1.0\n",
    "                except Exception as e:\n",
    "                    # If we can't check consistency, still reward frame count\n",
    "                    pass\n",
    "            \n",
    "            reward += 1.0  # Base reward for generating video\n",
    "        else:\n",
    "            # Not a list or empty - give minimal reward\n",
    "            reward = 0.0\n",
    "        \n",
    "        reward = max(-5.0, min(10.0, reward))\n",
    "        rewards.append(reward)\n",
    "    \n",
    "    # Debug: Print first reward\n",
    "    if len(rewards) > 0:\n",
    "        print(f\"üí∞ First reward: {rewards[0]:.2f}\")\n",
    "    \n",
    "    return rewards\n",
    "\n",
    "print(\"‚úÖ Video reward function created (with debugging)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Format Dataset for GRPO\n",
    "from datasets import Dataset\n",
    "\n",
    "def format_grpo_video(examples):\n",
    "    \"\"\"Format for GRPO training\"\"\"\n",
    "    prompts = []\n",
    "    for prompt in examples.get('prompt', examples.get('text', [])):\n",
    "        formatted = f\"Generate a video: {prompt}\"\n",
    "        prompts.append(formatted)\n",
    "    return {\"prompt\": prompts}\n",
    "\n",
    "grpo_dataset = dataset.map(format_grpo_video, batched=True)\n",
    "\n",
    "print(f\"‚úÖ GRPO dataset: {len(grpo_dataset)} examples\")\n",
    "print(f\"‚úÖ Example: {grpo_dataset[0]['prompt'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: GRPO Configuration\n",
    "from trl import GRPOConfig\n",
    "\n",
    "grpo_config = GRPOConfig(\n",
    "    output_dir=\"./text-to-video-grpo\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    max_steps=500,\n",
    "    warmup_steps=50,\n",
    "    bf16=torch.cuda.is_bf16_supported(),\n",
    "    fp16=not torch.cuda.is_bf16_supported(),\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    num_generations=4,  # Generate 4 videos per prompt\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "print(\"‚úÖ GRPO Config:\")\n",
    "print(f\"   Batch: {grpo_config.per_device_train_batch_size}\")\n",
    "print(f\"   Generations: {grpo_config.num_generations}\")\n",
    "print(f\"   Steps: {grpo_config.max_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Custom Video RL Training Loop\n",
    "# This implements GRPO for video generation\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "print(\"üöÄ Custom Video RL Training Loop\\n\")\n",
    "\n",
    "class VideoRLTrainer:\n",
    "    \"\"\"\n",
    "    Custom trainer for video RL fine-tuning\n",
    "    Implements GRPO for diffusion models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipe, reward_fn, config):\n",
    "        self.pipe = pipe\n",
    "        self.reward_fn = reward_fn\n",
    "        self.config = config\n",
    "        self.optimizer = AdamW(\n",
    "            self.pipe.unet.parameters(),\n",
    "            lr=config.learning_rate\n",
    "        )\n",
    "        self.step = 0\n",
    "        \n",
    "    def generate_videos(self, prompts, num_generations=4):\n",
    "        \"\"\"Generate multiple videos per prompt\"\"\"\n",
    "        all_videos = []\n",
    "        all_prompts = []\n",
    "        \n",
    "        for prompt in prompts:\n",
    "            videos = []\n",
    "            for _ in range(num_generations):\n",
    "                # Generate video\n",
    "                video = self.pipe(\n",
    "                    prompt,\n",
    "                    num_inference_steps=25,\n",
    "                ).frames[0]\n",
    "                videos.append(video)\n",
    "                all_prompts.append(prompt)\n",
    "            all_videos.extend(videos)\n",
    "        \n",
    "        return all_prompts, all_videos\n",
    "    \n",
    "    def train_step(self, prompts):\n",
    "        \"\"\"Single training step\"\"\"\n",
    "        # Generate videos\n",
    "        gen_prompts, videos = self.generate_videos(\n",
    "            prompts,\n",
    "            num_generations=self.config.num_generations\n",
    "        )\n",
    "        \n",
    "        # Score videos\n",
    "        rewards = self.reward_fn(prompts=gen_prompts, responses=videos)\n",
    "        \n",
    "        # Rank videos by reward\n",
    "        # Group by prompt\n",
    "        prompt_groups = {}\n",
    "        for i, prompt in enumerate(gen_prompts):\n",
    "            if prompt not in prompt_groups:\n",
    "                prompt_groups[prompt] = []\n",
    "            prompt_groups[prompt].append({\n",
    "                'video': videos[i],\n",
    "                'reward': rewards[i],\n",
    "                'index': i\n",
    "            })\n",
    "        \n",
    "        # For each prompt, find best video\n",
    "        best_rewards = []\n",
    "        for prompt in prompts:\n",
    "            if prompt in prompt_groups:\n",
    "                group = prompt_groups[prompt]\n",
    "                # Sort by reward\n",
    "                group.sort(key=lambda x: x['reward'], reverse=True)\n",
    "                best_reward = group[0]['reward']\n",
    "                best_rewards.append(best_reward)\n",
    "        \n",
    "        # Compute loss (simplified - reward maximization)\n",
    "        # In practice, use policy gradient or similar\n",
    "        avg_reward = sum(best_rewards) / len(best_rewards) if best_rewards else 0.0\n",
    "        \n",
    "        # Backward pass\n",
    "        # Note: This is simplified - real implementation needs proper gradient flow\n",
    "        loss = -avg_reward  # Maximize reward = minimize negative reward\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        \n",
    "        # For diffusion models, we need to backprop through generation\n",
    "        # This is complex - simplified version:\n",
    "        # Use the reward to weight the diffusion loss\n",
    "        # In practice, you'd need to implement policy gradient\n",
    "        \n",
    "        return {\n",
    "            'loss': loss,  # Already a float, no .item() needed\n",
    "            'avg_reward': avg_reward,\n",
    "            'rewards': rewards\n",
    "        }\n",
    "    \n",
    "    def train(self, dataset, max_steps=500):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(f\"üé¨ Starting Video RL Training ({max_steps} steps)...\\n\")\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Sample batch\n",
    "            batch = dataset.select(range(\n",
    "                step % len(dataset),\n",
    "                min(step % len(dataset) + self.config.per_device_train_batch_size, len(dataset))\n",
    "            ))\n",
    "            \n",
    "            prompts = [ex['prompt'] for ex in batch]\n",
    "            \n",
    "            # Training step\n",
    "            metrics = self.train_step(prompts)\n",
    "            \n",
    "            # Logging\n",
    "            if step % self.config.logging_steps == 0:\n",
    "                print(f\"Step {step}/{max_steps}:\")\n",
    "                print(f\"  Loss: {metrics['loss']:.4f}\")\n",
    "                print(f\"  Avg Reward: {metrics['avg_reward']:.4f}\")\n",
    "                print(f\"  Rewards: {metrics['rewards'][:4]}...\")\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if step % self.config.save_steps == 0 and step > 0:\n",
    "                save_path = f\"{self.config.output_dir}/checkpoint-{step}\"\n",
    "                os.makedirs(save_path, exist_ok=True)\n",
    "                self.pipe.save_pretrained(save_path)\n",
    "                print(f\"  üíæ Saved checkpoint to {save_path}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Training complete!\")\n",
    "\n",
    "# Create trainer\n",
    "trainer = VideoRLTrainer(\n",
    "    pipe=pipe,\n",
    "    reward_fn=video_quality_reward,\n",
    "    config=grpo_config\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Custom Video RL Trainer created!\")\n",
    "print(\"\\nüöÄ Ready to train! Run: trainer.train(grpo_dataset, max_steps=500)\")\n",
    "print(\"‚ö†Ô∏è Note: This is a simplified implementation\")\n",
    "print(\"   Full RL requires policy gradient or similar method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Start RL Training!\n",
    "# Run the training loop\n",
    "\n",
    "print(\"üöÄ Starting RL Fine-Tuning...\\n\")\n",
    "\n",
    "# Check if everything is ready\n",
    "if 'trainer' in globals() and 'grpo_dataset' in globals():\n",
    "    print(\"‚úÖ Trainer ready\")\n",
    "    print(\"‚úÖ Dataset ready\")\n",
    "    print(f\"‚úÖ Dataset size: {len(grpo_dataset)} examples\")\n",
    "    print(f\"‚úÖ Config: {grpo_config.max_steps} steps\\n\")\n",
    "    \n",
    "    print(\"‚ö†Ô∏è Important Notes:\")\n",
    "    print(\"   1. This is a simplified RL implementation\")\n",
    "    print(\"   2. Full RL requires proper policy gradient\")\n",
    "    print(\"   3. May need to adapt for diffusion models\")\n",
    "    print(\"   4. Training may take time\\n\")\n",
    "    \n",
    "    print(\"üí° To start training, uncomment the line below:\")\n",
    "    print(\"   trainer.train(grpo_dataset, max_steps=500)\")\n",
    "    \n",
    "    # Uncomment to start training:\n",
    "    # trainer.train(grpo_dataset, max_steps=500)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Setup incomplete. Run previous cells first.\")\n",
    "    print(\"   Make sure all cells above ran successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
