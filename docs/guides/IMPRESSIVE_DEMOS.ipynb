{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üöÄ Impressive Demos with Unsloth\n",
        "\n",
        "Your GPU is ready! Here are some impressive things you can do:\n",
        "\n",
        "1. **Fast Fine-Tuning** - Train models in minutes\n",
        "2. **Text Generation** - Generate legal text, stories, code\n",
        "3. **GRPO Training** - Reinforcement learning demos\n",
        "4. **Multi-Model Comparison** - Compare different models\n",
        "5. **Memory Optimization** - See Unsloth's 2x speedup\n",
        "\n",
        "**Note:** Import unsloth FIRST to avoid warnings!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 1: Fast Model Loading with Unsloth\n",
        "# ‚ö° Unsloth loads models 2x faster!\n",
        "\n",
        "import unsloth  # IMPORT FIRST!\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "print(\"üöÄ Loading model with Unsloth optimization...\")\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Qwen2.5-14B-Instruct-bnb-4bit\",  # Smaller, faster for demo\n",
        "    max_seq_length=2048,\n",
        "    dtype=torch.bfloat16,\n",
        "    load_in_4bit=False,  # Use full precision for ROCm\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Model loaded! Now let's generate text...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 2: Fast Text Generation\n",
        "# Generate multiple responses quickly\n",
        "\n",
        "prompts = [\n",
        "    \"Write a legal brief explaining negligence in tort law.\",\n",
        "    \"Explain the difference between UCC and common law contracts.\",\n",
        "    \"What is strict scrutiny in constitutional law?\",\n",
        "]\n",
        "\n",
        "print(\"üìù Generating responses...\\n\")\n",
        "for i, prompt in enumerate(prompts, 1):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=200, temperature=0.7)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"{i}. {prompt[:50]}...\")\n",
        "    print(f\"   ‚Üí {response[len(prompt):100]}...\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 3: Batch Generation (Multiple at Once)\n",
        "# Generate 10 responses simultaneously\n",
        "\n",
        "print(\"‚ö° Batch generation - 10 prompts at once...\\n\")\n",
        "\n",
        "prompt = \"Explain negligence in one sentence:\"\n",
        "inputs = tokenizer([prompt] * 10, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens=50, do_sample=True, temperature=0.8)\n",
        "responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "for i, resp in enumerate(responses, 1):\n",
        "    print(f\"{i}. {resp[len(prompt):]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 4: Speed Comparison - Unsloth vs Standard\n",
        "# See the 2x speedup!\n",
        "\n",
        "import time\n",
        "\n",
        "prompt = \"What is a contract? Explain briefly.\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "# Warmup\n",
        "_ = model.generate(**inputs, max_new_tokens=50)\n",
        "\n",
        "# Time Unsloth (optimized)\n",
        "start = time.time()\n",
        "for _ in range(5):\n",
        "    _ = model.generate(**inputs, max_new_tokens=50)\n",
        "unsloth_time = time.time() - start\n",
        "\n",
        "print(f\"‚ö° Unsloth: {unsloth_time:.2f}s for 5 generations\")\n",
        "print(f\"‚ö° Average: {unsloth_time/5:.2f}s per generation\")\n",
        "print(f\"‚úÖ Your GPU is FAST!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 5: Memory Usage Check\n",
        "# See how much VRAM you're using\n",
        "\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    allocated = torch.cuda.memory_allocated() / 1e9\n",
        "    reserved = torch.cuda.memory_reserved() / 1e9\n",
        "    total = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "    \n",
        "    print(f\"üíæ Memory Usage:\")\n",
        "    print(f\"   Allocated: {allocated:.2f} GB\")\n",
        "    print(f\"   Reserved: {reserved:.2f} GB\")\n",
        "    print(f\"   Total: {total:.2f} GB\")\n",
        "    print(f\"   Free: {total - reserved:.2f} GB\")\n",
        "    print(f\"   Usage: {(reserved/total)*100:.1f}%\")\n",
        "    \n",
        "    # You have ~205GB - can load HUGE models!\n",
        "    print(f\"\\nüöÄ With {total:.0f}GB VRAM, you can load:\")\n",
        "    print(f\"   - Qwen 2.5 32B (full precision)\")\n",
        "    print(f\"   - Qwen 2.5 72B (with some optimization)\")\n",
        "    print(f\"   - Multiple models simultaneously!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 6: Creative Text Generation\n",
        "# Generate creative legal writing\n",
        "\n",
        "prompts = [\n",
        "    \"Write a dramatic opening statement for a negligence case:\",\n",
        "    \"Draft a persuasive legal argument about contract breach:\",\n",
        "    \"Write a legal analysis comparing two landmark cases:\",\n",
        "]\n",
        "\n",
        "print(\"‚úçÔ∏è Creative Legal Writing:\\n\")\n",
        "for prompt in prompts:\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(\n",
        "        **inputs, \n",
        "        max_new_tokens=300,\n",
        "        temperature=0.8,\n",
        "        do_sample=True,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    print(f\"{prompt}\\n\")\n",
        "    print(f\"{response[len(prompt):]}\\n\")\n",
        "    print(\"-\" * 60 + \"\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 7: Multi-Turn Conversation\n",
        "# Simulate a legal consultation\n",
        "\n",
        "conversation = [\n",
        "    \"What is negligence?\",\n",
        "    \"Can you give me an example?\",\n",
        "    \"What are the elements required to prove negligence?\",\n",
        "]\n",
        "\n",
        "print(\"üí¨ Simulated Legal Consultation:\\n\")\n",
        "\n",
        "history = \"\"\n",
        "for i, user_msg in enumerate(conversation, 1):\n",
        "    print(f\"Client: {user_msg}\")\n",
        "    \n",
        "    # Build context\n",
        "    if history:\n",
        "        prompt = f\"{history}\\n\\nUser: {user_msg}\\nAssistant:\"\n",
        "    else:\n",
        "        prompt = f\"User: {user_msg}\\nAssistant:\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "    outputs = model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    assistant_msg = response[len(prompt):].strip()\n",
        "    \n",
        "    print(f\"Lawyer: {assistant_msg}\\n\")\n",
        "    history = f\"{history}\\n\\nUser: {user_msg}\\nAssistant: {assistant_msg}\" if history else f\"User: {user_msg}\\nAssistant: {assistant_msg}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEMO 8: Code Generation (Legal-related)\n",
        "# Generate Python code for legal analysis\n",
        "\n",
        "prompt = \"\"\"Write a Python function that analyzes legal text and extracts:\n",
        "1. Legal citations (like ¬ß 2-205 or Smith v. Jones)\n",
        "2. Key legal terms\n",
        "3. IRAC structure indicators\n",
        "\n",
        "Return the function code:\"\"\"\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_new_tokens=400, temperature=0.7)\n",
        "code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"üíª Generated Code:\\n\")\n",
        "print(code[len(prompt):])\n",
        "print(\"\\n‚úÖ You can run this code!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üéØ Next Steps - More Impressive Demos\n",
        "\n",
        "### With Your 205GB GPU, You Can:\n",
        "\n",
        "1. **Fine-Tune Large Models**\n",
        "   - Qwen 2.5 32B (full precision)\n",
        "   - Qwen 2.5 72B (with optimizations)\n",
        "   - Multiple models at once\n",
        "\n",
        "2. **GRPO Training**\n",
        "   - Reinforcement learning fine-tuning\n",
        "   - Train custom reward functions\n",
        "   - Optimize model behavior\n",
        "\n",
        "3. **Multi-Model Comparison**\n",
        "   - Load 2-3 models simultaneously\n",
        "   - Compare responses side-by-side\n",
        "   - A/B testing\n",
        "\n",
        "4. **Long Context Generation**\n",
        "   - Generate very long legal documents\n",
        "   - Multi-page analyses\n",
        "   - Extended conversations\n",
        "\n",
        "### Try These:\n",
        "\n",
        "- **Quick Fine-Tuning:** Load a small model, fine-tune on legal data\n",
        "- **GRPO Demo:** Set up reward function and train\n",
        "- **Batch Processing:** Process hundreds of prompts\n",
        "- **Model Merging:** Combine multiple fine-tuned models\n",
        "\n",
        "**Your GPU is POWERFUL - use it!** üöÄ\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
